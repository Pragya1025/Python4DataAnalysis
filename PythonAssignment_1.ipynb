{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 List of Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21,42,63,84,105,126,147,168,189,210,231,252,273,294,315,336,357,378,399,420,441,462,483\n"
     ]
    }
   ],
   "source": [
    "l=[]\n",
    "for i in range(1, 500):\n",
    "    if (i%3==0) and (i%7==0):\n",
    "        l.append(str(i))\n",
    "\n",
    "print (','.join(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Mean and Quartile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average element of the list is: 252.0\n"
     ]
    }
   ],
   "source": [
    "list =[21,42,63,84,105,126,147,168,189,210,231,252,273,294,315,336,357,378,399,420,441,462,483]\n",
    "sum = 0\n",
    "for i in list:\n",
    "    sum += i\n",
    "print (\"The average element of the list is: \" + str(sum/(len(list)*1.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136.5\n",
      "252.0\n",
      "367.5\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "list =[21,42,63,84,105,126,147,168,189,210,231,252,273,294,315,336,357,378,399,420,441,462,483]\n",
    "print (numpy.percentile(list,25))\n",
    "print (numpy.percentile(list,50))\n",
    "print (numpy.percentile(list,75))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Read the file ‘ques2.txt’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "more 1\n",
      "products 2\n",
      "best 1\n",
      "ideas 1\n",
      "software 1\n",
      "available.If 1\n",
      "innovative 1\n",
      "describe 2\n",
      "source 2\n",
      "a 5\n",
      "are 1\n",
      "wide 1\n",
      "through 1\n",
      "computation 3\n",
      "all 1\n",
      "between 2\n",
      "to 9\n",
      "while 1\n",
      "out 1\n",
      "share 1\n",
      "well.Data 1\n",
      "fast, 1\n",
      "variables. 1\n",
      "our 2\n",
      "future. 1\n",
      "for 3\n",
      "becomes 1\n",
      "in 9\n",
      "you 2\n",
      "implement 1\n",
      "create 1\n",
      "but 3\n",
      "really 1\n",
      "arrays, 1\n",
      "system 1\n",
      "products. 1\n",
      "keep 1\n",
      "assigned 1\n",
      "within 1\n",
      "its 1\n",
      "lacks 1\n",
      "By 1\n",
      "of 7\n",
      "exchanging 1\n",
      "purposes 1\n",
      "TensorFlowâ„¢ 1\n",
      "using 1\n",
      "Edges 1\n",
      "general 1\n",
      "operations, 2\n",
      "answer 1\n",
      "side 1\n",
      "implementations 1\n",
      "Machine 1\n",
      "why 1\n",
      "deep 1\n",
      "group 1\n",
      "API. 1\n",
      "learning 4\n",
      "asynchronously 1\n",
      "data, 1\n",
      "name. 1\n",
      "with 2\n",
      "results, 1\n",
      "one 2\n",
      "incoming 1\n",
      "computational 1\n",
      "as 1\n",
      "edges 3\n",
      "engineers 2\n",
      "Intelligence 1\n",
      "organization 1\n",
      "might 1\n",
      "Brain 1\n",
      "gets 1\n",
      "use 1\n",
      "toolboxes 1\n",
      "(tensors) 1\n",
      "can 1\n",
      "neural 1\n",
      "was 1\n",
      "conducting 1\n",
      "allows 1\n",
      "arrays 1\n",
      "networks 1\n",
      "user-facing 1\n",
      "so 1\n",
      "key 1\n",
      "be 2\n",
      "research, 1\n",
      "do 1\n",
      "devices 1\n",
      "technologies 1\n",
      "desktop, 1\n",
      "enough 1\n",
      "rather 1\n",
      "putting 1\n",
      "tools. 1\n",
      "originally 1\n",
      "typically 1\n",
      "the 13\n",
      "proprietary? 1\n",
      "global 1\n",
      "single 1\n",
      "edges. 1\n",
      "deploy 1\n",
      "researchers 1\n",
      "flow 3\n",
      "other 1\n",
      "also 1\n",
      "feed 1\n",
      "flexible 1\n",
      "TensorFlow 5\n",
      "numerical 1\n",
      "this 1\n",
      "The 3\n",
      "Team 1\n",
      "Google 2\n",
      "world, 1\n",
      "persistent 1\n",
      "& 1\n",
      "These 1\n",
      "endpoints 1\n",
      "what 1\n",
      "that 1\n",
      "data 4\n",
      "GPUs 1\n",
      "many 1\n",
      "them. 1\n",
      "graph 4\n",
      "where 1\n",
      "domains 1\n",
      "and 9\n",
      "CPUs 1\n",
      "open 3\n",
      "or 5\n",
      "mathematical 3\n",
      "parallel 1\n",
      "by 1\n",
      "device 1\n",
      "push 1\n",
      "ingredient 1\n",
      "think: 1\n",
      "it 2\n",
      "library 1\n",
      "execute 1\n",
      "their 1\n",
      "nodes. 1\n",
      "read/write 1\n",
      "applicable 1\n",
      "Research 1\n",
      "research 4\n",
      "great, 1\n",
      "on 2\n",
      "dynamically-sized 1\n",
      "variety 1\n",
      "multidimensional 2\n",
      "communicated 1\n",
      "graphs 1\n",
      "services, 1\n",
      "along 1\n",
      "hope 1\n",
      "growing 1\n",
      "than 2\n",
      "graphs. 1\n",
      "tensors 2\n",
      "we 2\n",
      "sharing 1\n",
      "Google's 1\n",
      "publications. 1\n",
      "standard 2\n",
      "an 2\n",
      "We 1\n",
      "server, 1\n",
      "Nodes 3\n",
      "machine 4\n",
      "represent 3\n",
      "is 7\n",
      "area 1\n",
      "architecture 1\n",
      "directed 1\n",
      "developed 1\n",
      "working 1\n",
      "tensors. 1\n",
      "mobile 1\n",
      "once 1\n",
      "nodes 1\n",
      "intends 1\n",
      "simpler 1\n",
      "relationships 1\n",
      "carry 1\n",
      "believe 2\n",
      "input/output 1\n"
     ]
    }
   ],
   "source": [
    "myfile = open(r'C:\\AED Assignment\\DataAnalysisPython\\Lecture-03\\ques2.txt', 'r+')\n",
    "wordcount={}\n",
    "for word in myfile.read().split():\n",
    "    if word not in wordcount:\n",
    "        wordcount[word] = 1\n",
    "    else:\n",
    "        wordcount[word] += 1\n",
    "for k,v in wordcount.items():\n",
    "    print (k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of words 309\n"
     ]
    }
   ],
   "source": [
    "myfile = open(r'C:\\AED Assignment\\DataAnalysisPython\\Lecture-03\\ques2.txt', 'r+')\n",
    "count=0\n",
    "for word in myfile.read().split():\n",
    "    count+=1\n",
    "print ('Numbers of words',count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Average, Mean,Median, Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean 5.624595469255663\n",
      "median 5\n",
      "mode 2\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "myfile = open(r'C:\\AED Assignment\\DataAnalysisPython\\Lecture-03\\ques2.txt', 'r+')\n",
    "index=0\n",
    "list=[]\n",
    "sum=0\n",
    "count=0\n",
    "for word in myfile.read().split():\n",
    "    list.insert(index,len(word))\n",
    "    index+=1\n",
    "for i in range (0,index):\n",
    "    sum+=list[i]\n",
    "avg = sum/index \n",
    "print ('Mean', avg)\n",
    "if (index +1)%2==0:\n",
    "    print ('median', list[round((index+1)/2-1)])\n",
    "else:\n",
    "    print ('median',(list[math.floor((index-1)/2)]+list[(math.cell((index-1)/2))])/2)    \n",
    "D=Counter(list)\n",
    "    \n",
    "for word, occ in D.items():\n",
    "    if occ == max(D.values()):\n",
    "        print('mode',word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 ‘User not found’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files 877\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "count=0\n",
    "suf=\"C:\\AED Assignment\\DataAnalysisPython\\Lecture-03\\jsons\"\n",
    "for filename in os.listdir(suf):\n",
    "    if filename.endswith(\".json\"):\n",
    "        with open(suf+'/'+filename) as data_file:\n",
    "            data = json.load(data_file)\n",
    "            if data.get(\"name\", 'none').upper()=='USER NOT FOUND':\n",
    "                count+=1\n",
    "print ('Number of files',count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Create a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random 5 entries:\n",
      "('G9pL2nBp1BMCcI2', 0)\n",
      "('Vztegpyo1AesYV5', 1)\n",
      "('vQ6jeTe31BMmzf2', 0)\n",
      "('zp4wY33w1BMpyW2', 0)\n",
      "('Qu1x0BYC1AHPoe5', 1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "count=0\n",
    "\n",
    "D={}\n",
    "D.clear()\n",
    "suf=\"C:\\AED Assignment\\DataAnalysisPython\\Lecture-03\\jsons\"\n",
    "for filename in os.listdir(suf):\n",
    "    if filename.endswith(\".json\"):\n",
    "        s=filename.split('.json')\n",
    "        \n",
    "        with open(suf+'/'+filename) as data_file:\n",
    "            data = json.load(data_file)\n",
    "            if data.get(\"name\",'none').upper()=='USER NOT FOUND':\n",
    "                D.update({s[0]: 0})\n",
    "               \n",
    "                \n",
    "            else:\n",
    "                D.update({s[0]: 1})\n",
    "\n",
    "print('Random 5 entries:')\n",
    "for i in range(5):\n",
    "    print(D.popitem())\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 JSON File `segmentUid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random 15 entries:\n",
      "('G9pL2nBp1BMCcI2', {'first': 0, 'third': 0})\n",
      "('Vztegpyo1AesYV5', {'first': 2, 'third': 7})\n",
      "('vQ6jeTe31BMmzf2', {'first': 0, 'third': 0})\n",
      "('zp4wY33w1BMpyW2', {'first': 0, 'third': 0})\n",
      "('Qu1x0BYC1AHPoe5', {'first': 152, 'third': 0})\n",
      "('A7PU2Z9H1BMMZ62', {'first': 0, 'third': 0})\n",
      "('rMUhNuX11AeS7d5', {'first': 121, 'third': 0})\n",
      "('rqIAinu01BDpqe5', {'first': 12, 'third': 41})\n",
      "('rKeUzrwj1ByL3k5', {'first': 8, 'third': 0})\n",
      "('Wt4ijZGh1zzCz65', {'first': 0, 'third': 0})\n",
      "('APADpQOO1BM4yi5', {'first': 0, 'third': 0})\n",
      "('VOLYlbqD1zz3jr5', {'first': 0, 'third': 0})\n",
      "('ROSNyFLA1zoWKP5', {'first': 0, 'third': 0})\n",
      "('RgL8LXyb1BuAUy5', {'first': 81, 'third': 145})\n",
      "('AzGRHSwv1A7SYr5', {'first': 0, 'third': 0})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "count=0\n",
    "D=[]\n",
    "segDict={}\n",
    "D.clear()\n",
    "fcount=0\n",
    "FinDict={}\n",
    "suf=\"C:\\AED Assignment\\DataAnalysisPython\\Lecture-03\\jsons\"\n",
    "for filename in os.listdir(suf):\n",
    "    if filename.endswith(\".json\"):\n",
    "        s=filename.split('.json')\n",
    "        count_0=0\n",
    "        count_1=0        \n",
    "        with open(suf+'/'+filename) as data_file:\n",
    "            IntDict={'first': 0,'third':0}\n",
    "            data = json.load(data_file)\n",
    "            if data.get(\"retargetingSegments\",'none')!='none':\n",
    "                D=data.get(\"retargetingSegments\",'none')\n",
    "                for i in range(0,len(D)):\n",
    "                    segDict=D[i]\n",
    "                    if segDict.get(\"segmentUid\")[0]=='0':\n",
    "                        count_0=count_0+1\n",
    "                        IntDict.update({'first': count_0})\n",
    "                        \n",
    "                        \n",
    "                    elif segDict.get(\"segmentUid\")[0]=='1':\n",
    "                        count_1=count_1+1\n",
    "                        IntDict.update({'third': count_1})\n",
    "                        \n",
    "                fcount=fcount+1   \n",
    "                    \n",
    "        FinDict.update({s[0]:IntDict})\n",
    "            \n",
    "#print(data)\n",
    "#print(D)\n",
    "#print(len(D))\n",
    "#print(count_0)\n",
    "#print(count_1)\n",
    "#print(FinDict)\n",
    "#print(fcount)\n",
    "print('Random 15 entries:')\n",
    "for i in range(15):\n",
    "    print(FinDict.popitem())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
